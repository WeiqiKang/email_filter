{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-23T11:39:47.022632500Z",
     "start_time": "2023-12-23T11:39:47.013083Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "def multi_line_to_single_line(input_string):\n",
    "    # 将字符串分割为多行\n",
    "    lines = input_string.splitlines()\n",
    "\n",
    "    # 使用列表推导式处理每行，并去除非英文字符（保留空格和英文字符）\n",
    "    processed_lines = [''.join(char for char in line if char.isalpha() or char.isspace()) for line in lines]\n",
    "\n",
    "    # 将处理后的行连接成一个单行字符串\n",
    "    single_line_string = ' '.join(processed_lines)\n",
    "\n",
    "    return single_line_string\n",
    "\n",
    "\n",
    "\n",
    "# 数据预处理操作（词的切分、词转化为小写）\n",
    "def text_parse(input_str):\n",
    "    input_str = multi_line_to_single_line(input_str)\n",
    "    word_list = input_str.split()\n",
    "    return [word.lower() for word in word_list if len(word_list) > 2 and len(word) > 0]  # 字母都转换成小写，过滤掉长度为0的单词\n",
    "\n",
    "\n",
    "# 获取数据\n",
    "def read_data():\n",
    "    doc_list = [] # 每句话的单词列表\n",
    "    class_list = []  # 每句话是否是垃圾邮件，1表示是\n",
    "\n",
    "    df = pd.read_csv(\"./data/SECD.csv\")\n",
    "    header = df.columns.values.tolist()\n",
    "    datas = df.values.tolist()\n",
    "\n",
    "    for data in datas:\n",
    "        label, text = data[0], data[1]\n",
    "        doc_list.append(text_parse(text))\n",
    "        class_list.append(label)\n",
    "\n",
    "    return np.array(doc_list), np.array(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 构建语料表，把所有单词整合在一个集合中\n",
    "def create_vocabulary_list(doc_list):\n",
    "    vocabulary_set = set([])\n",
    "    for document in doc_list:\n",
    "        vocabulary_set = vocabulary_set | set(document)\n",
    "    return list(vocabulary_set)\n",
    "\n",
    "\n",
    "# 将一篇邮件转化为 类似 One-Hot 的向量，长度和 vocabulary_list 一样，为 1 的位置代表该单词在该邮件中出现了\n",
    "def set_of_word2vector(vocabulary_list, document):\n",
    "    vec = [0 for _ in range(len(vocabulary_list))]  # 先生成一个和vocabulary_list长度一样的，全0的向量\n",
    "    for word in document:\n",
    "        index = vocabulary_list.index(word)  # 当前这句话的这个单词在vocabulary_list的第几个位置出现了，在vec的对应位置标记为1\n",
    "        if index >= 0:\n",
    "            vec[index] = 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "# 用朴素贝叶斯算法进行计算\n",
    "def naive_bayes(train_matrix, train_class):\n",
    "    # 训练样本个数\n",
    "    train_data_size = len(train_class)\n",
    "    # 语料库大小，总数据个数，即向量长度\n",
    "    vocabulary_size = len(train_matrix[0])\n",
    "    # 计算垃圾邮件的概率值\n",
    "    p_spam = sum(train_class) / train_data_size\n",
    "    # 初始化分子，做了一个平滑处理（拉普拉斯平滑）\n",
    "    p_ham_molecule = np.ones(vocabulary_size)\n",
    "    p_spam_molecule = np.ones(vocabulary_size)\n",
    "\n",
    "    # 初始化分母（通常初始化为类别个数，在垃圾邮件分类中，只有垃圾和正常两种邮件，所以类别数为2）\n",
    "    p_ham_denominator = 2\n",
    "    p_spam_denominator = 2\n",
    "    # 循环计算分子和分母\n",
    "    for i in range(train_data_size):\n",
    "        if train_class[i] == 1:\n",
    "            p_spam_molecule += train_matrix[i]\n",
    "            p_spam_denominator += sum(train_matrix[i])\n",
    "        else:\n",
    "            p_ham_molecule += train_matrix[i]\n",
    "            p_ham_denominator += sum(train_matrix[i])\n",
    "    # 计算概率\n",
    "    p_ham_vec = p_ham_molecule / p_ham_denominator\n",
    "    p_spam_vec = p_spam_molecule / p_spam_denominator\n",
    "    # 返回\n",
    "    return p_ham_vec, p_spam_vec, p_spam\n",
    "\n",
    "\n",
    "# 预测，返回预测的类别\n",
    "def predict(vec, p_ham_vec, p_spam_vec, p_spam):\n",
    "    # 由于计算出来的概率通常很接近0，所以我们通常取对数将它“放大”，这也基于我们在做贝叶斯的时候，不需要知道他们确切的概率，只需要比较他们概率大小即可\n",
    "    p_spam = np.log(p_spam) + sum(vec * np.log(p_spam_vec))\n",
    "    p_ham = np.log(1 - p_spam) + sum(vec * np.log(p_ham_vec))\n",
    "    return 1 if p_spam >= p_ham else 0\n",
    "\n",
    "\n",
    "def get_newdata_doclist(datas):\n",
    "    new_data_doclist = []\n",
    "    datas = datas.split('\\n')\n",
    "    for data in datas:\n",
    "        new_data_doclist = new_data_doclist + text_parse(data)\n",
    "    return new_data_doclist"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T11:39:48.532601Z",
     "start_time": "2023-12-23T11:39:48.516557300Z"
    }
   },
   "id": "3712dbd69650cba6",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 83448 email data were read, including 43910 spam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwq\\anaconda3\\envs\\mail_filter\\lib\\site-packages\\ipykernel_launcher.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "doc_list, class_list = read_data()\n",
    "print(f\"A total of {len(class_list)} email data were read, including {sum(class_list)} spam\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T11:40:01.380486500Z",
     "start_time": "2023-12-23T11:39:50.043483100Z"
    }
   },
   "id": "d5989588faa1023f",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-9-b83d6869ca51>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# 构建语料表\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mvocabulary_list\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_vocabulary_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-7-f1e297ee7bb2>\u001B[0m in \u001B[0;36mcreate_vocabulary_list\u001B[1;34m(doc_list)\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mvocabulary_set\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mdocument\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdoc_list\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m         \u001B[0mvocabulary_set\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvocabulary_set\u001B[0m \u001B[1;33m|\u001B[0m \u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdocument\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvocabulary_set\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 构建语料表\n",
    "vocabulary_list = create_vocabulary_list(doc_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T11:45:05.515727200Z",
     "start_time": "2023-12-23T11:43:26.263834200Z"
    }
   },
   "id": "e026a9be41d51eab",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f23ac86acd20b82a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
